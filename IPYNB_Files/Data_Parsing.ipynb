{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing \n",
    "\n",
    "## The following cells will be debugging and development for a data parsing script utilizing pandas and system commands to access, create, and organize files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, datetime, glob, os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x,y,z,a,b: datetime.datetime.strptime(f\"{x} {y} {z} {a} {b}\", \"%Y %m %d %H %M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"..\\\\Data\\\\\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_file = pd.read_csv(\"../NREL Data/2007_data/f6e76f02d6341e8d2299b9af8966bd69/30_18.40_-67.23_2007.csv\", header=[2],parse_dates={'Date':[0,1,2,3,4]},date_parser=custom_date_parser)\n",
    "display_file2 = pd.read_csv(\n",
    "    \"../NREL Data/2007_data/f6e76f02d6341e8d2299b9af8966bd69/30_18.40_-67.23_2007.csv\", nrows=1)\n",
    "display_file3 = pd.read_csv(\"../NREL Data/2007_data/f6e76f02d6341e8d2299b9af8966bd69/120_18.32_-67.15_2007.csv\", header=[2],parse_dates={'Date':[0,1,2,3,4]},date_parser=custom_date_parser)\n",
    "display_file4 = pd.read_csv(\n",
    "    \"../NREL Data/2007_data/f6e76f02d6341e8d2299b9af8966bd69/120_18.32_-67.15_2007.csv\", nrows=1)\n",
    "display_file[\"Latitude\"] = display_file2.Latitude[0]\n",
    "display_file[\"Longitude\"] = display_file2.Longitude[0]\n",
    "display_file3[\"Latitude\"] = display_file4.Latitude[0]\n",
    "display_file3[\"Longitude\"] = display_file4.Longitude[0]\n",
    "test_df = pd.concat([display_file,display_file3])\n",
    "test_df\n",
    "test_group_df = test_df.groupby([pd.Grouper(key=\"Date\", freq='1D'),\"Latitude\",\"Longitude\"]).mean()\n",
    "test_group_df\n",
    "# desired_output = pd.read_csv(\"../Solar Data/INSOLRICO.2021302.csv\", delim_whitespace=True,names=[\"value\",\"latitude\",\"longitude\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"2007\"\n",
    "files = glob.glob(f\"../NREL Data/**/*{year}.csv\",recursive=True)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Year = \"2007\"\n",
    "BigDF = pd.DataFrame()\n",
    "dateparse = lambda x: pd.datetime.strptime(x, '%Y %m %d %H %M')\n",
    "#Store all of the files as a list, in order to access it in the loop appropriately. \n",
    "files = glob.glob(f\"../NREL Data/**/*{year}.csv\",recursive=True)\n",
    "for index in range(0,len(files)):\n",
    "    if len(files)>2:\n",
    "        DataFile = files.pop()\n",
    "        DataFile2 = files.pop()\n",
    "        Csv_DF = pd.read_csv(DataFile, header=[2], parse_dates={'Date': [0, 1, 2, 3, 4]}, \n",
    "                             date_parser=dateparse)\n",
    "        Lat_Lon_DF1 = pd.read_csv(DataFile, nrows=1)\n",
    "        Csv_DF[\"Longitude\"] = Lat_Lon_DF1.Longitude[0]\n",
    "        Csv_DF[\"Latitude\"] = Lat_Lon_DF1.Latitude[0]\n",
    "        Csv_DF2 = pd.read_csv(DataFile2, header=[2], parse_dates={\n",
    "                              'Date': [0, 1, 2, 3, 4]}, date_parser=dateparse)\n",
    "        Lat_Lon_DF2 = pd.read_csv(DataFile2, nrows=1)\n",
    "        Csv_DF2[\"Longitude\"] = Lat_Lon_DF2.Longitude[0]\n",
    "        Csv_DF2[\"Latitude\"] = Lat_Lon_DF2.Latitude[0]\n",
    "        BigDF = BigDF.append([Csv_DF,Csv_DF2])\n",
    "    elif len(files)==1:\n",
    "        DataFile = files.pop()\n",
    "        Csv_DF = pd.read_csv(DataFile, header=[2], parse_dates={\n",
    "                             'Date': [0, 1, 2, 3, 4]}, date_parser=dateparse)\n",
    "        Lat_Lon_DF1 = pd.read_csv(DataFile, nrows=1)\n",
    "        Csv_DF[\"Longitude\"] = Lat_Lon_DF1.Longitude[0]\n",
    "        Csv_DF[\"Latitude\"] = Lat_Lon_DF1.Latitude[0]\n",
    "        BigDF = BigDF.append([Csv_DF])\n",
    "\n",
    "Parsed_Data = BigDF.groupby([pd.Grouper(key=\"Date\", freq='1D'),\"Latitude\",\"Longitude\"]).mean()\n",
    "Parsed_Data = Parsed_Data.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parsed_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_date = datetime.date(int(Year), 1, 1)\n",
    "day_offset = datetime.timedelta(days=1)\n",
    "Values_To_Parse = [\"DHI\",\"GHI\", \"DNI\",\"Air Temperature\"]\n",
    "for Value in Values_To_Parse:\n",
    "    Data = pd.DataFrame(columns=['value', 'latitude', 'longitude'])\n",
    "    for rowIndex, row in Parsed_Data.iterrows():\n",
    "        if rowIndex[0].date() == expected_date:\n",
    "            Data = Data.append({'value': row[Value], 'latitude': \n",
    "                rowIndex[1], 'longitude': rowIndex[2]}, ignore_index=True)\n",
    "        elif expected_date.year > rowIndex[0].date().year and expected_date.day >= 1:\n",
    "            expected_date = datetime.date(int(Year),1,1)\n",
    "            continue\n",
    "        else:\n",
    "            Data.to_csv(\n",
    "                path_or_buf=f\"{data_dir}{rowIndex[0].strftime('%Y')}{Value}/{Value}{expected_date.strftime('%Y%j')}.csv\", \n",
    "                header=False, index=False, sep=' ')\n",
    "            Data.drop(index=Data.index, inplace=True)\n",
    "            expected_date = expected_date+day_offset\n",
    "    print(Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_date = datetime.date(2007,1,1)\n",
    "day_offset = datetime.timedelta(days=1)\n",
    "Parsed_Data.index[0][0].date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"2007\"\n",
    "BigDF = pd.DataFrame()\n",
    "files = glob.glob(f\"../NREL Data/**/*{year}.csv\",recursive=True)\n",
    "for index in range(0,len(files)):\n",
    "    if len(files)>2:\n",
    "        DataFile = files.pop()\n",
    "        DataFile2 = files.pop()\n",
    "        CSV_DF = pd.read_csv(DataFile,header=[2],parse_dates={'Date':[0,1,2,3,4]},date_parser=custom_date_parser)\n",
    "        Lat_Lon_DF1 = pd.read_csv(DataFile, nrows=1)\n",
    "        CSV_DF[\"Longitude\"] = Lat_Lon_DF1.Longitude[0]\n",
    "        CSV_DF[\"Latitude\"] = Lat_Lon_DF1.Latitude[0]\n",
    "        CSV_DF2 = pd.read_csv(DataFile2,header=[2],parse_dates={'Date':[0,1,2,3,4]},date_parser=custom_date_parser)\n",
    "        Lat_Lon_DF2 = pd.read_csv(DataFile2, nrows=1)\n",
    "        CSV_DF2[\"Longitude\"] = Lat_Lon_DF2.Longitude[0]\n",
    "        CSV_DF2[\"Latitude\"] = Lat_Lon_DF2.Latitude[0]\n",
    "        BigDF = BigDF.append([CSV_DF,CSV_DF2])\n",
    "    elif len(files)==1:\n",
    "        DataFile = files.pop()\n",
    "        CSV_DF = pd.read_csv(DataFile,header=[2],parse_dates={'Date':[0,1,2,3,4]},date_parser=custom_date_parser)\n",
    "        Lat_Lon_DF1 = pd.read_csv(DataFile, nrows=1)\n",
    "        CSV_DF[\"Longitude\"] = Lat_Lon_DF1.Longitude[0]\n",
    "        CSV_DF[\"Latitude\"] = Lat_Lon_DF1.Latitude[0]\n",
    "        BigDF = BigDF.append([CSV_DF])\n",
    "else:\n",
    "    print(len(files))\n",
    "    BigDF = BigDF.groupby([pd.Grouper(key=\"Date\", freq='1D'),\"Latitude\",\"Longitude\"]).mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BigDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group_df2 = BigDF.groupby([pd.Grouper(level=\"Date\", freq='1D'),\"Latitude\",\"Longitude\"]).mean()\n",
    "test_group_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = display2.iloc[:1]\n",
    "d2 = display2.iloc[2:]\n",
    "# display2.iloc[1]\n",
    "\n",
    "## Next step is figure out a possible lambda function to create the new date column and datetime object after which dataframe will be ready to move on to the next step of parsing.\n",
    "\n",
    "d2 = d2.rename(columns=display2.loc[1]).dropna(axis=1)\n",
    "d2\n",
    "d2['Date'] = pd.to_datetime(d2['Year']+\" \"+d2['Month']+\" \"+d2['Day']+\" \"+d2['Hour']+\" \"+d2['Minute'],format=\"%Y %m %d %H %M\")\n",
    "\n",
    "d2 = d2.drop(columns=['Year','Month','Day','Hour','Minute'])\n",
    "\n",
    "\n",
    "## research about possible list slicing for automating the reorganizing. Evaluate if truly necessary\n",
    "cols = list(d2.columns)\n",
    "cols = [cols[-1]]  + cols[:-1]\n",
    "d2 = d2[cols]\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rowIndex, row in  test_group_df.iterrows():\n",
    "    # print(rowIndex[1],rowIndex[2],row['GHI'])\n",
    "    GHIData = pd.DataFrame(data={'value': [row['GHI']],'Latitude': [rowIndex[1]],'Longitude': [rowIndex[2]]})\n",
    "    AirTempData = pd.DataFrame(data={'value': [row['Air Temperature']],'Latitude': [rowIndex[1]],'Longitude': [rowIndex[2]]})\n",
    "    DNIData = pd.DataFrame(data={'value': [row['DNI']],'Latitude': [rowIndex[1]],'Longitude': [rowIndex[2]]})\n",
    "    WindSpeedData = pd.DataFrame(data={'value': [row['Wind Speed']],'Latitude': [rowIndex[1]],'Longitude': [rowIndex[2]]})\n",
    "    # RelHumData = pd.DataFrame(data={'value': [row['Relative Humidity']],'Latitude': [rowIndex[1]],'Longitude': [rowIndex[2]]})\n",
    "    GHIData.to_csv(path_or_buf=f\"../Test Data/GHI/GHI{rowIndex[0].strftime('%Y%j')}\",header=False,index=False,sep=' ')\n",
    "    AirTempData.to_csv(path_or_buf=f\"../Test Data/AirTemp/AirTemperature{rowIndex[0].strftime('%Y%j')}\",header=False,index=False,sep=' ')\n",
    "    DNIData .to_csv(path_or_buf=f\"../Test Data/DNI/DNI{rowIndex[0].strftime('%Y%j')}\",header=False,index=False,sep=' ')\n",
    "    WindSpeedData.to_csv(path_or_buf=f\"../Test Data/WindSpeed/WindSpeed{rowIndex[0].strftime('%Y%j')}\",header=False,index=False,sep=' ')\n",
    "    # RelHumData.to_csv(path_or_buf=f\"../Test Data/RelHum/RelativeHumidity{rowIndex[0].strftime('%Y%j')}\",header=False,index=False,sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"..\\\\Data\\\\\"\n",
    "\n",
    "\n",
    "def make_dir(Year,Directory_names):\n",
    "    \"\"\"\n",
    "    Will take in a list of the directory names to create and then will proceed to generate the directories\n",
    "     _____________________________________\n",
    "\n",
    "    Parameters: \n",
    "\n",
    "    Directory_names:  This is a list that must contain all of the names that will be used to create the desired directories within the Data directory.\n",
    "    \"\"\"\n",
    "\n",
    "    for name in Directory_names:\n",
    "        if not os.path.isdir(data_dir+Year+name):\n",
    "            print(data_dir+Year+name)\n",
    "            os.makedirs(data_dir+Year+name)\n",
    "        else:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dir(\"2019\",[\"GHI\",\"DNI\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
